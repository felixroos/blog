{"componentChunkName":"component---node-modules-gatsby-theme-blog-core-src-templates-post-query-js","path":"/web-audio-recorder/","result":{"data":{"site":{"siteMetadata":{"title":"Loophole Letters","social":[{"name":"github","url":"https://github.com/felixroos"},{"name":"GitHub","url":"https://github.com/gatsbyjs"}]}},"blogPost":{"__typename":"MdxBlogPost","id":"c5dc26e8-7436-5b02-a016-b4030ee1d635","excerpt":"I made a few experiments on Audio Recording, using the Web Audio API. First, I created a simple project: ... added some html: ... and a…","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Recording Audio on the Web\",\n  \"date\": \"2020-10-27T00:00:00.000Z\",\n  \"image\": \"./img/waveform.png\",\n  \"keywords\": [\"web audio\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(Recorder, {\n    mdxType: \"Recorder\"\n  }), mdx(\"p\", null, \"I made a few experiments on Audio Recording, using the Web Audio API.\"), mdx(\"details\", null, mdx(\"summary\", null, \"show project setup\"), mdx(\"p\", null, \"First, I created a simple project:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-sh\"\n  }, \"mkdir recorder && cd recorder && npm init # init project\\nnpm i parcel --save-dev # install parcel to serve files\\nnpm i react react-dom --save # install react for ui\\ntouch index.html app.js # create files\\nparcel index.html --open # start server & open in browser\\n\")), mdx(\"p\", null, \"\\u2026 added some html:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-html\"\n  }, \"<html>\\n  <head>\\n    <title>Recorder</title>\\n  </head>\\n  <body>\\n    <div id=\\\"app\\\" />\\n    <script src=\\\"./app.js\\\"></script>\\n  </body>\\n</html>\\n\")), mdx(\"p\", null, \"\\u2026 and a minimal react setup:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"import React from 'react';\\nimport ReactDOM from 'react-dom';\\n\\nconst App = () => {\\n  return <button>record</button>;\\n};\\nconst appContainer = document.querySelector('#app');\\nReactDOM.render(<App />, appContainer);\\n\")), mdx(\"p\", null, \"Now we\\u2019re ready to go!\")), mdx(\"br\", null), mdx(\"h2\", {\n    \"id\": \"recording-audio\"\n  }, \"Recording Audio\"), mdx(\"h3\", {\n    \"id\": \"useusermedia-hook\"\n  }, \"useUserMedia hook\"), mdx(\"p\", null, \"At first, we need to get access to the users microphone.\\nWe can do this with \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia\"\n  }, \"getUserMedia\"), \", wrapped into a hook:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"import React, { useState } from 'react';\\nimport ReactDOM from 'react-dom';\\nimport 'regenerator-runtime/runtime'; // https://flaviocopes.com/parcel-regeneratorruntime-not-defined/\\n\\nfunction useUserMedia(constraints) {\\n  const [stream, setStream] = useState();\\n  function getStream(refresh = false) {\\n    if (stream && !refresh) {\\n      return stream;\\n    }\\n    return navigator.mediaDevices.getUserMedia(constraints).then((_stream) => {\\n      setStream(_stream);\\n      return _stream;\\n    });\\n  }\\n  return { stream, getStream };\\n}\\n\")), mdx(\"h3\", {\n    \"id\": \"usemediarecorder-hook\"\n  }, \"useMediaRecorder hook\"), mdx(\"p\", null, \"To record audio, we can now pass our stream to the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder\"\n  }, \"MediaRecorder API\"), \", also wrapped with a hook:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"export function useMediaRecorder({ onStart, onStop, onData }: { onStart?: any, onStop?: any, onData?: any }) {\\n  const [recorder, setRecorder] = useState<any>();\\n  const [state, setState] = useState<string>('inactive');\\n  const { getStream } = useUserMedia({ audio: true, video: false });\\n  const audioChunks = useRef([]); // this will contain the recorded chunks\\n  async function start(timeslices?, _stream?) {\\n    const stream = _stream || await getStream(true); // request stream using our custom hook\\n    audioChunks.current = [];\\n    const _recorder = new MediaRecorder(stream);\\n    onStart && onStart(_recorder);\\n    _recorder.start(timeslices); // start recording with timeslices\\n    setRecorder(_recorder);\\n    setState(_recorder.state);\\n    // called every timeslices (ms)\\n    _recorder.addEventListener('dataavailable', (event) => {\\n      audioChunks.current.push(event.data);\\n      onData && onData(event, audioChunks.current);\\n    });\\n    _recorder.addEventListener('stop', () => {\\n      onStop && onStop(audioChunks.current);\\n      setState(_recorder.state);\\n    });\\n  }\\n  async function stop() {\\n    if (recorder) {\\n      recorder.stop();\\n      (await getStream()).getTracks().forEach(track => track.stop());\\n    }\\n  }\\n  return { start, stop, state };\\n}\\n\\n\")), mdx(\"h3\", {\n    \"id\": \"usage-in-the-app\"\n  }, \"Usage in the App\"), mdx(\"p\", null, \"We can now use the above hook and play the recorded audio when we press stop:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"const App = () => {\\n  const { start, stop } = useMediaRecorder({\\n    constraints: { audio: true, video: false }, // audio only\\n    onStop: (audioChunks) => {\\n      const audioBlob = new Blob(audioChunks);\\n      const audioUrl = URL.createObjectURL(audioBlob);\\n      const audio = new Audio(audioUrl);\\n      audio.play();\\n    },\\n  });\\n  return (\\n    <>\\n      <button onClick={() => start()}>record</button>\\n      <button onClick={() => stop()}>stop</button>\\n    </>\\n  );\\n};\\n\")), mdx(\"p\", null, \"Result:\"), mdx(Recorder, {\n    hideWaveform: true,\n    mdxType: \"Recorder\"\n  }), mdx(\"h2\", {\n    \"id\": \"displaying-an-audio-waveform\"\n  }, \"Displaying an audio waveform\"), mdx(\"h3\", {\n    \"id\": \"getting-the-raw-pcm-data\"\n  }, \"Getting the raw PCM data\"), mdx(\"p\", null, \"First, we need to the raw pcm data:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"function getPCM(blob) {\\n  return new Promise((resolve, reject) => {\\n    const fileReader = new FileReader();\\n    fileReader.onloadend = () => {\\n      const arrayBuffer = fileReader.result;\\n      // Convert array buffer into audio buffer\\n      audioContext.decodeAudioData(arrayBuffer, (audioBuffer) => {\\n        // Do something with audioBuffer\\n        const pcm = audioBuffer.getChannelData(0);\\n        resolve(pcm);\\n      });\\n    };\\n    fileReader.onerror = reject;\\n    fileReader.readAsArrayBuffer(blob);\\n  });\\n}\\n\")), mdx(\"h3\", {\n    \"id\": \"drawing-the-waveform\"\n  }, \"Drawing the Waveform\"), mdx(\"p\", null, \"Let\\u2019s set up the canvas:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"export function Waveform({ pcm, playhead }) {\\n  const [canvasRef, setCanvasRef] = useState();\\n\\n  useEffect(() => {\\n    if (pcm && canvasRef) {\\n      drawPCM(pcm, canvasRef, playhead); // TODO\\n    }\\n  }, [pcm, canvasRef, playhead]);\\n\\n  function prettyCanvas(width, height, style) {\\n    return {\\n      width: width * 2,\\n      height: height * 2,\\n      style: { width, height, ...style },\\n    };\\n  }\\n  return <canvas ref={setCanvasRef} {...prettyCanvas(640, 200, { backgroundColor: '#BFBFBF' })} />;\\n}\\n\")), mdx(\"p\", null, \"Now we can draw the waveform like this:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"function drawPCM(values, canvas, playhead) {\\n  const ctx = canvas.getContext('2d');\\n  let { width: clientWidth, height: clientHeight } = canvas;\\n  canvas.width = clientWidth;\\n  const scale = 2;\\n  ctx.scale(scale, scale);\\n  clientWidth /= scale; // scale down for pretty canvas\\n  clientHeight /= scale;\\n  const absoluteValues = true; // if false, we will retain the true waveform\\n  const valuesPerPixel = values.length / clientWidth;\\n  const blockSize = 1; // width of one sample block\\n  let max = 0;\\n  const averageValues = [];\\n  for (let x = 0; x < clientWidth; x += blockSize) {\\n    const area = values.slice(Math.floor(x * valuesPerPixel), Math.ceil((x + blockSize) * valuesPerPixel));\\n    const areaReducer = absoluteValues ? (sum, v) => sum + Math.abs(v) : (sum, v) => sum + v;\\n    const value = area.reduce(areaReducer, 0) / area.length;\\n    max = max < value ? value : max;\\n    averageValues.push(value);\\n  }\\n  averageValues.forEach((value, index) => {\\n    const height = (((value / max) * clientHeight) / 2) * 0.9;\\n    ctx.beginPath();\\n    ctx.strokeStyle = `#3535C3`;\\n    ctx.fillStyle = `#6464D8`;\\n    const args = [index * blockSize, clientHeight / 2 - (absoluteValues ? height / 2 : 0), blockSize, height];\\n    const borderRadius = Math.floor(Math.min(args[2], args[3]) / 2);\\n    ctx.fillRect(index * blockSize, clientHeight / 2 - (absoluteValues ? height / 2 : 0), blockSize, height);\\n    ctx.stroke();\\n  });\\n  if (playhead) {\\n    ctx.beginPath();\\n    const x = playhead * clientWidth;\\n    ctx.moveTo(x, 0);\\n    ctx.lineTo(x, clientHeight);\\n    ctx.stroke();\\n  }\\n}\\n\")), mdx(\"p\", null, \"In our app, we can now update the pcm on stop and render the waveform:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-js\"\n  }, \"const App = () => {\\n  const [pcm, setPcm] = useState();\\n  const { start, stop } = useMediaRecorder({\\n    onStop: async (audioChunks) => {\\n      const audioBlob = new Blob(audioChunks);\\n      setPcm(await getPCM(audioBlob));\\n      /* other stuff */\\n    },\\n  });\\n  return (\\n    <>\\n      {pcm && <Waveform pcm={pcm} />}\\n      {/* other stuff*/}\\n    </>\\n  );\\n};\\n\")), mdx(\"p\", null, \"For the \\u201Cother stuff\\u201D I used some Material UI components to make it sweeter:\"), mdx(Recorder, {\n    mdxType: \"Recorder\"\n  }), mdx(\"p\", null, \"That\\u2019s it. I will go into using analyzer nodes in a future post!\"), mdx(\"h2\", {\n    \"id\": \"links\"\n  }, \"Links\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bbc/peaks.js\"\n  }, \"https://github.com/bbc/peaks.js\"), \" + \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://waveform.prototyping.bbc.co.uk/\"\n  }, \"https://waveform.prototyping.bbc.co.uk/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://getstream.io/blog/generating-waveforms-for-podcasts-in-winds-2-0/\"\n  }, \"https://getstream.io/blog/generating-waveforms-for-podcasts-in-winds-2-0/\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://kenhoff.github.io/winds-waveform-example/\"\n  }, \"https://kenhoff.github.io/winds-waveform-example/\")))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://css-tricks.com/making-an-audio-waveform-visualizer-with-vanilla-javascript/\"\n  }, \"https://css-tricks.com/making-an-audio-waveform-visualizer-with-vanilla-javascript/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://library-demos.herokuapp.com/react-voice-recorder\"\n  }, \"https://library-demos.herokuapp.com/react-voice-recorder\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/danrouse/react-audio-recorder\"\n  }, \"https://github.com/danrouse/react-audio-recorder\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://hackingbeauty.github.io/react-mic/\"\n  }, \"https://hackingbeauty.github.io/react-mic/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://ruebel.github.io/waveform-react/\"\n  }, \"https://ruebel.github.io/waveform-react/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://wavesurfer-js.org/\"\n  }, \"https://wavesurfer-js.org/\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.npmjs.com/package/react-wavesurfer\"\n  }, \"https://www.npmjs.com/package/react-wavesurfer\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://dev.to/jamland/audio-player-with-wavesurfer-js-react-1g3b\"\n  }, \"https://dev.to/jamland/audio-player-with-wavesurfer-js-react-1g3b\")))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.freecodecamp.org/news/how-to-make-realtime-soundcloud-waveforms-in-react-native-4df0f4c6b3cc/\"\n  }, \"https://www.freecodecamp.org/news/how-to-make-realtime-soundcloud-waveforms-in-react-native-4df0f4c6b3cc/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/klambycom/react-waveform\"\n  }, \"https://github.com/klambycom/react-waveform\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/andrewrk/waveform\"\n  }, \"https://github.com/andrewrk/waveform\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.twilio.com/blog/audio-visualisation-web-audio-api--react\"\n  }, \"https://www.twilio.com/blog/audio-visualisation-web-audio-api\\u2014react\"))));\n}\n;\nMDXContent.isMDXComponent = true;","slug":"/web-audio-recorder/","title":"Recording Audio on the Web","tags":[],"keywords":["web audio"],"date":"October 27, 2020","image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAAArUlEQVQY061Puw7BUAC9P2z1AzaLTSwY2BgaCRaJ0HhEaYu0NCj1aF2UtiE0aXtUf0AkTnKm88oh+IIgCPALiDQ3cbNeKDELyIoJbefA83zYjosjfUSmmXJFk91jG2rt7gFsyE9OnJzR4wxUaioSyQHyRRmE4yku5hOZ3BRDgWKpWvD9AJbtQjfuUeFIpFFordlgqiuUw3F+fEKnr6Pe2CBbkBCLt5BKCyD/vvwGTZ4rRhaVkBIAAAAASUVORK5CYII=","aspectRatio":4.545454545454546,"src":"/blog/static/07e79a980cb018a4ad5760c2c07da764/ee604/waveform.png","srcSet":"/blog/static/07e79a980cb018a4ad5760c2c07da764/69585/waveform.png 200w,\n/blog/static/07e79a980cb018a4ad5760c2c07da764/497c6/waveform.png 400w,\n/blog/static/07e79a980cb018a4ad5760c2c07da764/ee604/waveform.png 800w,\n/blog/static/07e79a980cb018a4ad5760c2c07da764/f3583/waveform.png 1200w,\n/blog/static/07e79a980cb018a4ad5760c2c07da764/80b7d/waveform.png 1294w","sizes":"(max-width: 800px) 100vw, 800px"}}},"imageAlt":null,"socialImage":null},"previous":{"__typename":"MdxBlogPost","id":"44906244-bde8-5211-8609-c087f7b866d8","excerpt":"I always get nostalgia when listening to the soundtracks of the  SNES , for example: I transcribed many of those  here Chipsynth SFC…","slug":"/chipsynth-sfc/","title":"Recreating SNES Music with Chipsynth SFC","date":"October 20, 2020"},"next":{"__typename":"MdxBlogPost","id":"ac2b32d3-ca18-596f-89a7-db1f04563617","excerpt":"With the Web MIDI API, we can send and receive MIDI events from javascript.\nThis allows us to trigger notes on any instrument! Turning MIDI…","slug":"/webmidi/","title":"Using MIDI with JavaScript","date":"December 30, 2020"}},"pageContext":{"id":"c5dc26e8-7436-5b02-a016-b4030ee1d635","previousId":"44906244-bde8-5211-8609-c087f7b866d8","nextId":"ac2b32d3-ca18-596f-89a7-db1f04563617"}},"staticQueryHashes":["386998304","4198970465","764694655"]}